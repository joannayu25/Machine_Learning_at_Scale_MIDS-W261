{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5Q8 - How to run in GCP\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__\n",
    "\n",
    "This notebook contains supplemental materials to help you run your HW5 solution to question 8 using Google Compute Platform. __Important Note:__ _the graders will not read this notebook. If you do use it, please be sure to copy relevant output back into the main homework notebook to receive credit for your results._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account setup\n",
    "1. Create your GCP account & apply for credit through the w261 education grant. (see [create_account.md](https://github.com/UCB-w261/w261-environment/blob/master/gcp/account-setup/create_account.md))\n",
    "2. Set up your project, bucket, service account, access key and virtual environment. (steps 1-15 in [setup.md](https://github.com/UCB-w261/w261-environment/blob/master/gcp/account-setup/setup.md))\n",
    "3. (OPTIONAL) Review the GCP documentation to become more familiar with the setup steps you've just performed: [key terms & concepts described here](https://cloud.google.com/storage/docs/concepts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the submission Script\n",
    "Copy the [submit_job_to_cluster.py](https://github.com/UCB-w261/w261-environment/blob/master/gcp/dataproc/submit_job_to_cluster.py) file from the environment repo into your current working directory. This script will help you run your own spark jobs on the cluster. You can read more about it here: [w261-environment](https://github.com/UCB-w261/w261-environment/tree/master/gcp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure to give your script executable permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x submit_job_to_cluster.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the data to your gcp bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To copy files from dropbox to google storage run:   \n",
    "`curl -L file-url | gsutil cp - gs://bucket-name/filename.txt`   \n",
    "\n",
    "For example, to stream the whole wiki graph from dropbox into my bucket, I would run:   \n",
    "`curl -L \"https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAD7I_6kQlJtDpXZPhCfVH-a/wikipedia/all-pages-indexed-out.txt?dl=0\" | gsutil cp - gs://w261-bucket/wiki_graph.txt`\n",
    "\n",
    "* To copy files from your computer to google storage, run:   \n",
    "`gsutil cp 'data/test_graph.txt' gs://bucket-name/test_graph.txt`   \n",
    "\n",
    "__IMPORTANT:__ You will need to run this outside of the Docker container, as the container doesn't have `gsutil` installed.\n",
    "\n",
    "For additonal information about moving files to your GS bucket, see: https://www.cloudbooklet.com/gsutil-cp-copy-and-move-files-on-google-cloud/   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run a spark job on a cluster using GCP\n",
    "Fill in your PageRank code, and run the cell below to create a file called `pagerank.py` in the current directory.    \n",
    "__IMPORTANT:__ Make sure and fill in your own Bucket Name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pagerank.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pagerank.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([ ('spark.executor.pyspark.memory', '11g'), ('spark.driver.memory','11g')])\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "#sc = pyspark.SparkContext()\n",
    "\n",
    "\n",
    "############## YOUR BUCKET HERE ###############\n",
    "\n",
    "BUCKET=\"jyu-mids-w261-2019fall\"\n",
    "\n",
    "############## (END) YOUR BUCKET ###############\n",
    "\n",
    "\n",
    "wikiRDD = sc.textFile(\"gs://\"+BUCKET+\"/wiki_graph.txt\")\n",
    "\n",
    "\n",
    "def initGraph(dataRDD):\n",
    "    \"\"\"\n",
    "    Spark job to read in the raw data and initialize an \n",
    "    adjacency list representation with a record for each\n",
    "    node (including dangling nodes).\n",
    "    \n",
    "    Returns: \n",
    "        graphRDD -  a pair RDD of (node_id , (score, edges))\n",
    "        \n",
    "    NOTE: The score should be a float, but you may want to be \n",
    "    strategic about how format the edges... there are a few \n",
    "    options that can work. Make sure that whatever you choose\n",
    "    is sufficient for Question 8 where you'll run PageRank.\n",
    "    \"\"\"\n",
    "    ############## YOUR CODE HERE ###############\n",
    "    \n",
    "    # write any helper functions here\n",
    "    \n",
    "    # Tokenize each line and emit a pair RDD in the \n",
    "    # (node_id , edges) format for all nodes\n",
    "    # including: 1) nodes with real edges, 2) dangling nodes\n",
    "    # where the 'edges' list is set to the empty dict {}. \n",
    "    def emit_nodes(line):\n",
    "        node, edges = line.split('\\t')\n",
    "        edge_nodes = ast.literal_eval(edges)\n",
    "        all_edges = ''\n",
    "        for edge_node in edge_nodes.keys():\n",
    "            weight=edge_nodes[edge_node]\n",
    "           # all_edges=all_edges+','+edge_node+','*(weight-1)+edge_node*(weight-1)\n",
    "            yield (edge_node, '')\n",
    "            yield (node, edge_node+(','+edge_node)*(weight-1) )\n",
    "      #  yield (node, all_edges)\n",
    "\n",
    "    def combineEdges(x, y):\n",
    "        if x=='': return y\n",
    "        elif y=='': return x\n",
    "        else: return x+','+y\n",
    "\n",
    "    # write your main Spark code here\n",
    "    \n",
    "    # For all nodes in the data, emit (node_id , edges).\n",
    "    # Use 'reduceByKey' to take out duplicates and cache \n",
    "    # this 'tempRDD'.\n",
    "    tempRDD = dataRDD.flatMap(emit_nodes) \\\n",
    "                     .reduceByKey(combineEdges) \\\n",
    "                     .cache()\n",
    "    \n",
    "    # Compute N by calling count() on 'tempRDD' and broadcast \n",
    "    # the value so the mappers can access it.\n",
    "    totalCount = tempRDD.count()    \n",
    "    init_value = sc.broadcast(1/totalCount)\n",
    "    \n",
    "    # Initalize the correct score (1/N) using the newly computed N. \n",
    "    graphRDD = tempRDD.map(lambda line: (line[0], (init_value.value, line[1])))\n",
    "   \n",
    "    ############## (END) YOUR CODE ###############\n",
    "    \n",
    "    return graphRDD\n",
    "\n",
    "class FloatAccumulatorParam(AccumulatorParam):\n",
    "    \"\"\"\n",
    "    Custom accumulator for use in page rank to keep track of various masses.\n",
    "    \n",
    "    IMPORTANT: accumulators should only be called inside actions to avoid duplication.\n",
    "    We stringly recommend you use the 'foreach' action in your implementation below.\n",
    "    \"\"\"\n",
    "    def zero(self, value):\n",
    "        return value\n",
    "    def addInPlace(self, val1, val2):\n",
    "        return val1 + val2\n",
    "    \n",
    "def runPageRank(graphInitRDD, alpha = 0.15, maxIter = 10, verbose = True):\n",
    "    \"\"\"\n",
    "    Spark job to implement page rank\n",
    "    Args: \n",
    "        graphInitRDD  - pair RDD of (node_id , (score, edges))\n",
    "        alpha         - (float) teleportation factor\n",
    "        maxIter       - (int) stopping criteria (number of iterations)\n",
    "        verbose       - (bool) option to print logging info after each iteration\n",
    "    Returns:\n",
    "        steadyStateRDD - pair RDD of (node_id, pageRank)\n",
    "    \"\"\"\n",
    "    # teleportation:\n",
    "    a = sc.broadcast(alpha)\n",
    "    \n",
    "    # damping factor:\n",
    "    d = sc.broadcast(1-a.value)\n",
    "    \n",
    "    # initialize accumulators for dangling mass & total mass\n",
    "    mmAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n",
    "    totAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n",
    "    \n",
    "    ############## YOUR CODE HERE ###############\n",
    "   \n",
    "    # write your helper functions here, \n",
    "    # please document the purpose of each clearly \n",
    "    # for reference, the master solution has 5 helper functions.\n",
    "\n",
    "    # Mapper function to emit probability masses for each node's \n",
    "    # neighbors. \n",
    "    def emit_prob_mass(line):\n",
    "        # Parse the line, tokenize each element, and convert the\n",
    "        # corresponding dictionary back to dict format. \n",
    "      #  print('in emit')\n",
    "        node, (prob, edges) = line        \n",
    "        yield node, (0, edges)\n",
    "        \n",
    "        \n",
    "        # For each item in the neighboring node dictionary, calculate\n",
    "        # the probability mass being redistributed and emit in the format\n",
    "        # of (node_id, (score, {})) for each neighbor. To preserve the graph \n",
    "        # structure for the next iteration, we also emit the original node \n",
    "        # as (node_id, (0, {neighbors})).\n",
    "        if edges != '': \n",
    "            edge_list = edges.split(',')\n",
    "            total_wt = len(edge_list)\n",
    "            for edge_node in edge_list:\n",
    "                yield edge_node, (prob/total_wt, '')\n",
    "    \n",
    "    def inc_mmAccum(line):\n",
    "        node, (rank, edges) = line\n",
    "        if edges == '': \n",
    "            mmAccum.add(rank)\n",
    "\n",
    "    def update_rank(line):\n",
    "        node, (rank, edges) = line\n",
    "        new_rank = teleport_val.value + d.value*(mmass_per_node+rank)\n",
    "       ## print(\"newrank\", new_rank)\n",
    "      #  totAccum.add(new_rank)\n",
    "        yield node, (new_rank, edges)        \n",
    "        \n",
    "    def not_converged(RDD_A, RDD_B):\n",
    "        if RDD_A == None or RDD_B == None: \n",
    "            return True\n",
    "        else: \n",
    "            for key in RDD_A.keys().collect():\n",
    "                if np.absolute(RDD_A.lookup(key)[0][0]-RDD_B.lookup(key)[0][0]) > threshold.value:\n",
    "                    return True\n",
    "            return False\n",
    "       \n",
    "    # write your main Spark Job here (including the for loop to iterate)\n",
    "    # for reference, the master solution is 21 lines including comments & whitespace\n",
    "\n",
    "    # Initialize variables before the while loop. 'lastRDD' and 'currentRDD' are \n",
    "    # used to keep track of convergence. 'NAccum' is to keep track of N. 'Iteration'\n",
    "    # is to keep track of the number of iterations so far. 'threshold' is the \n",
    "    # difference between the probabilities that defines convergence. \n",
    "    #lastRDD = None \n",
    "    steadyStateRDD = graphInitRDD\n",
    "    iteration = 0\n",
    "    N = sc.broadcast(graphInitRDD.count())\n",
    "  #  threshold = sc.broadcast(1e-18)\n",
    "    # The main while loop that keeps iteration going until either achieving\n",
    "    # 'maxIter' or the probabilities of the next state changes less than \n",
    "    # the 'threshold' value. \n",
    "    while (maxIter > iteration):# and not_converged(lastRDD, currentRDD)):  \n",
    "        print('starting while loop', time.time())\n",
    "        # Call inc_mmAccum to increment the missing mass and N accumulators.\n",
    "        steadyStateRDD.foreach(inc_mmAccum)\n",
    "\n",
    "        teleport_val = sc.broadcast(np.divide(a.value,N.value))\n",
    "        \n",
    "        mmass_per_node = sc.broadcast(np.divide(mmAccum.value,N.value))\n",
    "        # Use mappers to emit probability masses and use reducers to combine\n",
    "        # the probability masses.\n",
    "        print('starting sparkjob', time.time())\n",
    "        steadyStateRDD = steadyStateRDD.flatMap(emit_prob_mass) \\\n",
    "                                       .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "                                       .flatMap(update_rank)\\\n",
    "                                       .cache()\n",
    "        print('starting tot', time.time())\n",
    "        steadyStateRDD.foreach(lambda line: totAccum.add(line[1][0]))\n",
    "        \n",
    "        # Compute the teleport portion of the rank and missing mass per node and\n",
    "        # store these as broadcast variables for efficiency. Compute the rank  \n",
    "        # and increment 'totAccum' by each rank to ensure the total rank is 1.\n",
    "\n",
    "        if verbose:\n",
    "            print('--->STEP {}: missing mass = {}, total = {}'.\n",
    "                  format(iteration, mmAccum.value, totAccum.value))\n",
    "       \n",
    "        # Prepare for the next iteration by incrementing 'interation' \n",
    "        # and resetting all accumulators.\n",
    "        iteration += 1\n",
    "        mmAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n",
    "        totAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n",
    "  \n",
    "    # Make a new RDD with the nodes and probability masses.\n",
    "    steadyStateRDD = steadyStateRDD.map(lambda line: (line[0], line[1][0]))\n",
    "    \n",
    "    ############## (END) YOUR CODE ###############\n",
    "    \n",
    "    return steadyStateRDD\n",
    "\n",
    "nIter = 10\n",
    "start = time.time()\n",
    "\n",
    "# Initialize your graph structure (Q7)\n",
    "wikiGraphRDD = initGraph(wikiRDD)\n",
    "print('finished Q7')\n",
    "\n",
    "# Run PageRank (Q8)\n",
    "full_results = runPageRank(wikiGraphRDD, alpha = 0.15, maxIter = nIter, verbose = True)\n",
    "\n",
    "print(f'...trained {nIter} iterations in {time.time() - start} seconds.')\n",
    "print(f'Top 20 ranked nodes:')\n",
    "print(full_results.takeOrdered(20, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this command in your terminal (Not in the Docker container!), to submit your job to GCP. You will need to have your environment variables pre-defined. Alterantively, substitute them with the actual values.   \n",
    "\n",
    "* PROJECT_ID: your GCP project id   \n",
    "* BUCKET_NAME: the name of your GCP bucket   \n",
    "* CLUSTER_NAME: choose a cluster name, this should include only a-z, 0-9 & start with a letter   \n",
    "* ZONE: The zone for your account and bucket, ex: us-central1-b\n",
    "\n",
    "\n",
    "```\n",
    "python3 submit_job_to_cluster.py \\\n",
    "    --project_id=${PROJECT_ID} \\\n",
    "    --zone=${ZONE} \\\n",
    "    --cluster_name=${CLUSTER_NAME} \\\n",
    "    --gcs_bucket=${BUCKET_NAME} \\\n",
    "    --key_file=$HOME/w261.json \\\n",
    "    --create_new_cluster \\\n",
    "    --pyspark_file=pagerank.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-cb3a3fd564c7>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-cb3a3fd564c7>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ```\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "```\n",
    "python3 submit_job_to_cluster.py \\\n",
    "    --project_id='w261-fall-2019' \\\n",
    "    --zone='us-west2-a' \\\n",
    "    --cluster_name='w261hw5q8' \\\n",
    "    --gcs_bucket='jyu-mids-w261-2019fall' \\\n",
    "    --key_file=$HOME/w261.json \\\n",
    "    --create_new_cluster \\\n",
    "    --pyspark_file=pagerank.py \\\n",
    "    --worker_nodes=5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='w261-fall-2019'\n",
    "ZONE='us-west2-a'\n",
    "CLUSTER_NAME='w261hw5q8'\n",
    "BUCKET_NAME='jyu-mids-w261-2019fall'"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
