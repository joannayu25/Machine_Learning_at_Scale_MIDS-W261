{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5Q8 - How to run in GCP\n",
    "__`MIDS w261: Machine Learning at Scale | UC Berkeley School of Information | Fall 2018`__\n",
    "\n",
    "This notebook contains supplemental materials to help you run your HW5 solution to question 8 using Google Compute Platform. __Important Note:__ _the graders will not read this notebook. If you do use it, please be sure to copy relevant output back into the main homework notebook to receive credit for your results._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account setup\n",
    "1. Create your GCP account & apply for credit through the w261 education grant. (see [create_account.md](https://github.com/UCB-w261/w261-environment/blob/master/gcp/account-setup/create_account.md))\n",
    "2. Set up your project, bucket, service account, access key and virtual environment. (steps 1-15 in [setup.md](https://github.com/UCB-w261/w261-environment/blob/master/gcp/account-setup/setup.md))\n",
    "3. (OPTIONAL) Review the GCP documentation to become more familiar with the setup steps you've just performed: [key terms & concepts described here](https://cloud.google.com/storage/docs/concepts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the submission Script\n",
    "Copy the [submit_job_to_cluster.py](https://github.com/UCB-w261/w261-environment/blob/master/gcp/dataproc/submit_job_to_cluster.py) file from the environment repo into your current working directory. This script will help you run your own spark jobs on the cluster. You can read more about it here: [w261-environment](https://github.com/UCB-w261/w261-environment/tree/master/gcp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure to give your script executable permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x submit_job_to_cluster.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the data to your gcp bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To copy files from dropbox to google storage run:   \n",
    "`curl -L file-url | gsutil cp - gs://bucket-name/filename.txt`   \n",
    "\n",
    "For example, to stream the whole wiki graph from dropbox into my bucket, I would run:   \n",
    "`curl -L \"https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAD7I_6kQlJtDpXZPhCfVH-a/wikipedia/all-pages-indexed-out.txt?dl=0\" | gsutil cp - gs://w261-bucket/wiki_graph.txt`\n",
    "\n",
    "* To copy files from your computer to google storage, run:   \n",
    "`gsutil cp 'data/test_graph.txt' gs://bucket-name/test_graph.txt`   \n",
    "\n",
    "__IMPORTANT:__ You will need to run this outside of the Docker container, as the container doesn't have `gsutil` installed.\n",
    "\n",
    "For additonal information about moving files to your GS bucket, see: https://www.cloudbooklet.com/gsutil-cp-copy-and-move-files-on-google-cloud/   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and run a spark job on a cluster using GCP\n",
    "Fill in your PageRank code, and run the cell below to create a file called `pagerank.py` in the current directory.    \n",
    "__IMPORTANT:__ Make sure and fill in your own Bucket Name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pagerank.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pagerank.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([ ('spark.executor.pyspark.memory', '11g'), ('spark.driver.memory','11g')])\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "#sc = pyspark.SparkContext()\n",
    "\n",
    "\n",
    "############## YOUR BUCKET HERE ###############\n",
    "\n",
    "BUCKET=\"jyu-mids-w261-2019fall\"\n",
    "\n",
    "############## (END) YOUR BUCKET ###############\n",
    "\n",
    "\n",
    "wikiRDD = sc.textFile(\"gs://\"+BUCKET+\"/wiki_graph.txt\")\n",
    "\n",
    "\n",
    "def initGraph(dataRDD):\n",
    "    \"\"\"\n",
    "    Spark job to read in the raw data and initialize an \n",
    "    adjacency list representation with a record for each\n",
    "    node (including dangling nodes).\n",
    "    \n",
    "    Returns: \n",
    "        graphRDD -  a pair RDD of (node_id , (score, edges))\n",
    "        \n",
    "    NOTE: The score should be a float, but you may want to be \n",
    "    strategic about how format the edges... there are a few \n",
    "    options that can work. Make sure that whatever you choose\n",
    "    is sufficient for Question 8 where you'll run PageRank.\n",
    "    \"\"\"\n",
    "    ############## YOUR CODE HERE ###############\n",
    "    \n",
    "    # write any helper functions here\n",
    "    \n",
    "    # Tokenize each line and emit a pair RDD in the \n",
    "    # (node_id , edges) format for all nodes\n",
    "    # including: 1) nodes with real edges, 2) dangling nodes\n",
    "    # where the 'edges' list is set to the empty string. \n",
    "    # This implementation represent the neighbors as strings \n",
    "    # delimited by commas.\n",
    "    def emit_nodes(line):\n",
    "        node, edges = line.split('\\t')\n",
    "        edge_nodes = ast.literal_eval(edges)\n",
    "        all_edges = ''\n",
    "        for edge_node in edge_nodes.keys():\n",
    "            weight=edge_nodes[edge_node]\n",
    "            yield (edge_node, '')\n",
    "            yield (node, edge_node+(','+edge_node)*(weight-1) )\n",
    "\n",
    "    def combineEdges(x, y):\n",
    "        if x=='': return y\n",
    "        elif y=='': return x\n",
    "        else: return x+','+y\n",
    "\n",
    "    # write your main Spark code here\n",
    "    \n",
    "    # For all nodes in the data, emit (node_id , edges).\n",
    "    # Use 'reduceByKey' to take out duplicates and cache \n",
    "    # this 'tempRDD'.\n",
    "    tempRDD = dataRDD.flatMap(emit_nodes) \\\n",
    "                     .reduceByKey(combineEdges) \\\n",
    "                     .cache()\n",
    "    \n",
    "    # Compute N by calling count() on 'tempRDD' and broadcast \n",
    "    # the value so the mappers can access it.\n",
    "    totalCount = tempRDD.count()    \n",
    "    init_value = sc.broadcast(1/totalCount)\n",
    "    \n",
    "    # Initalize the correct score (1/N) using the newly computed N. \n",
    "    graphRDD = tempRDD.map(lambda line: (line[0], (init_value.value, line[1])))\n",
    "   \n",
    "    ############## (END) YOUR CODE ###############\n",
    "    \n",
    "    return graphRDD\n",
    "\n",
    "class FloatAccumulatorParam(AccumulatorParam):\n",
    "    \"\"\"\n",
    "    Custom accumulator for use in page rank to keep track of various masses.\n",
    "    \n",
    "    IMPORTANT: accumulators should only be called inside actions to avoid duplication.\n",
    "    We stringly recommend you use the 'foreach' action in your implementation below.\n",
    "    \"\"\"\n",
    "    def zero(self, value):\n",
    "        return value\n",
    "    def addInPlace(self, val1, val2):\n",
    "        return val1 + val2\n",
    "    \n",
    "def runPageRank(graphInitRDD, alpha = 0.15, maxIter = 10, verbose = True):\n",
    "    \"\"\"\n",
    "    Spark job to implement page rank\n",
    "    Args: \n",
    "        graphInitRDD  - pair RDD of (node_id , (score, edges))\n",
    "        alpha         - (float) teleportation factor\n",
    "        maxIter       - (int) stopping criteria (number of iterations)\n",
    "        verbose       - (bool) option to print logging info after each iteration\n",
    "    Returns:\n",
    "        steadyStateRDD - pair RDD of (node_id, pageRank)\n",
    "    \"\"\"\n",
    "    # teleportation:\n",
    "    a = sc.broadcast(alpha)\n",
    "    \n",
    "    # damping factor:\n",
    "    d = sc.broadcast(1-a.value)\n",
    "    \n",
    "    # initialize accumulators for dangling mass & total mass\n",
    "    mmAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n",
    "    totAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n",
    "    \n",
    "    ############## YOUR CODE HERE ###############\n",
    "   \n",
    "    \n",
    "    # write your helper functions here, \n",
    "    # please document the purpose of each clearly \n",
    "    # for reference, the master solution has 5 helper functions.\n",
    "\n",
    "    # Mapper function to emit probability masses for each node's \n",
    "    # neighbors. \n",
    "    def emit_prob_mass(line):\n",
    "        # Parse the line, tokenize each element, and emit the \n",
    "        # original record with 0 rank, (node_id, (0, {neighbors})),\n",
    "        # to preserve the graph structure for the next iteration.\n",
    "        node, (prob, edges) = line        \n",
    "        yield node, (0, edges)        \n",
    "        # For each neighboring node, calculate the probability mass \n",
    "        # being redistributed and emit in the format of (node_id, (score, {})) \n",
    "        # for each neighbor. Initially, I emitted one record per \n",
    "        # edge, so the repeated edges to the same neighbor were emitted \n",
    "        # multiple times. That would flood the stream but could leverage \n",
    "        # Spark's reduceByKey. The implementation below combines the \n",
    "        # repeated edges so fewer records are emitted to the stream, \n",
    "        # but more computation is done in this function. Both implementation \n",
    "        # were tested and combining repeated edges before emitting is slightly \n",
    "        # faster.\n",
    "        if edges != '': \n",
    "            edge_list = edges.split(',')\n",
    "            total_wt = len(edge_list)\n",
    "            current = edge_list[0]\n",
    "            weight=0\n",
    "            # Iterate through the list of edges and emit the combined\n",
    "            # probability once a new neighbor is encountered.  \n",
    "            for i in range(total_wt):\n",
    "                if edge_list[i]==current: \n",
    "                    weight+=1\n",
    "                else:        \n",
    "                    yield current, (prob/total_wt*weight, '')\n",
    "                    weight=1\n",
    "                    current=edge_list[i]\n",
    "            if weight!=0: yield current, (prob/total_wt*weight, '')\n",
    "            \n",
    "    # Increment the accumulators (mmAccum and totAccum).\n",
    "    def inc_accum(line):\n",
    "        node, (rank, edges) = line\n",
    "        totAccum.add(rank)\n",
    "        if edges == '': \n",
    "            mmAccum.add(rank)\n",
    "        \n",
    "    # Computes the new rank by applying the formula.\n",
    "    def update_rank(line):\n",
    "        node, (rank, edges) = line\n",
    "        new_rank = teleport_val.value + d.value*(mmass_per_node.value+rank)\n",
    "        yield node, (new_rank, edges)        \n",
    "        \n",
    "    # Checks convergence given a threshold value broadcasted by the driver.\n",
    "    # This function is not used since it is not required. \n",
    "    def not_converged(RDD_A, RDD_B):\n",
    "        if RDD_A == None or RDD_B == None: \n",
    "            return True\n",
    "        else: \n",
    "            for key in RDD_A.keys().collect():\n",
    "                if np.absolute(RDD_A.lookup(key)[0][0]-RDD_B.lookup(key)[0][0]) > threshold.value:\n",
    "                    return True\n",
    "            return False\n",
    "       \n",
    "    # write your main Spark Job here (including the for loop to iterate)\n",
    "    # for reference, the master solution is 21 lines including comments & whitespace\n",
    "\n",
    "    # Initialize variables before the while loop. 'Iteration' is to keep track \n",
    "    # of the number of iterations so far. 'threshold' is the difference between \n",
    "    # the probabilities that defines convergence, but it was not used.\n",
    "\n",
    "    steadyStateRDD = graphInitRDD\n",
    "    iteration = 0\n",
    "    threshold = sc.broadcast(1e-10) \n",
    "    N = graphInitRDD.count()\n",
    "    # Initialize mmAccum\n",
    "    graphInitRDD.filter(lambda line: line[1][1]=='') \\\n",
    "                .foreach(lambda line: mmAccum.add(line[1][0])) \n",
    "    # The main while loop that keeps iteration going until either achieving\n",
    "    # 'maxIter' or the probabilities of the next state changes less than \n",
    "    # the 'threshold' value. \n",
    "    while (maxIter > iteration):\n",
    "\n",
    "        # Compute and broadcast relevant parameters for the mappers to use\n",
    "        # in applying the PageRank formula.\n",
    "        teleport_val = sc.broadcast(np.divide(a.value,N))\n",
    "        m = mmAccum.value\n",
    "        mmass_per_node = sc.broadcast(np.divide(m,N))#\n",
    "        \n",
    "        # Use mappers to emit probability masses and use reducers to combine\n",
    "        # the probability masses. And finally use mappers to perform PageRank \n",
    "        # computation and cache the RDD.\n",
    "        steadyStateRDD = steadyStateRDD.flatMap(emit_prob_mass) \\\n",
    "                                       .reduceByKey(lambda x, y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "                                       .flatMap(update_rank)\\\n",
    "                                       .cache()\n",
    "\n",
    "        # Reset mmAccum to zero and then initialize it based on the current\n",
    "        # iteration's values. \n",
    "        mmAccum = sc.accumulator(0.0, FloatAccumulatorParam()) \n",
    "        steadyStateRDD.foreach(inc_accum)\n",
    "\n",
    "        if verbose:\n",
    "            print('--->STEP {}: missing mass = {}, total = {}'.\n",
    "                  format(iteration, m, totAccum.value))\n",
    "       \n",
    "        # Prepare for the next iteration by incrementing 'interation' \n",
    "        # and resetting totAccum.\n",
    "        iteration += 1\n",
    "        totAccum = sc.accumulator(0.0, FloatAccumulatorParam())\n",
    "  \n",
    "    # Make a new RDD with the nodes and probability masses only.\n",
    "    steadyStateRDD = steadyStateRDD.map(lambda line: (line[0], line[1][0]))\n",
    "    \n",
    "    ############## (END) YOUR CODE ###############\n",
    "    \n",
    "    return steadyStateRDD\n",
    "\n",
    "#testRDD = sc.textFile(\"gs://\"+BUCKET+\"/test_graph.txt\")##\n",
    "#nIter = 20\n",
    "#testGraphRDD = initGraph(testRDD)\n",
    "#start = time.time()\n",
    "#test_results = runPageRank(testGraphRDD, alpha = 0.15, maxIter = nIter, verbose = True) ## verbose defaults to f\n",
    "#print('...trained {} iterations in {} seconds.'.format(nIter,time.time() - start ))\n",
    "#print('Top 20 ranked nodes:')\n",
    "#test_results.takeOrdered(20, key=lambda x: - x[1])\n",
    "\n",
    "\n",
    "\n",
    "nIter = 10\n",
    "start = time.time()\n",
    "\n",
    "# Initialize your graph structure (Q7)\n",
    "wikiGraphRDD = initGraph(wikiRDD)\n",
    "print('finished Q7')\n",
    "#print('Total number of records: {}'.format(wikiGraphRDD.count()))\n",
    "#print('First record: {}'.format(wikiGraphRDD.take(1)))\n",
    "\n",
    "# Run PageRank (Q8)\n",
    "full_results = runPageRank(wikiGraphRDD, alpha = 0.15, maxIter = nIter, verbose = True)\n",
    "\n",
    "print(f'...trained {nIter} iterations in {time.time() - start} seconds.')\n",
    "print(f'Top 20 ranked nodes:')\n",
    "#print('...trained {} iterations in {} seconds.'.format(nIter,time.time() - start ))\n",
    "#print('Top 20 ranked nodes:')\n",
    "print(full_results.takeOrdered(20, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this command in your terminal (Not in the Docker container!), to submit your job to GCP. You will need to have your environment variables pre-defined. Alterantively, substitute them with the actual values.   \n",
    "\n",
    "* PROJECT_ID: your GCP project id   \n",
    "* BUCKET_NAME: the name of your GCP bucket   \n",
    "* CLUSTER_NAME: choose a cluster name, this should include only a-z, 0-9 & start with a letter   \n",
    "* ZONE: The zone for your account and bucket, ex: us-central1-b\n",
    "\n",
    "\n",
    "```\n",
    "python3 submit_job_to_cluster.py \\\n",
    "    --project_id=${PROJECT_ID} \\\n",
    "    --zone=${ZONE} \\\n",
    "    --cluster_name=${CLUSTER_NAME} \\\n",
    "    --gcs_bucket=${BUCKET_NAME} \\\n",
    "    --key_file=$HOME/w261.json \\\n",
    "    --create_new_cluster \\\n",
    "    --pyspark_file=pagerank.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-cb3a3fd564c7>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-cb3a3fd564c7>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ```\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "```\n",
    "python3 submit_job_to_cluster.py \\\n",
    "    --project_id='w261-fall-2019' \\\n",
    "    --zone='us-west2-a' \\\n",
    "    --cluster_name='w261hw5q8' \\\n",
    "    --gcs_bucket='jyu-mids-w261-2019fall' \\\n",
    "    --key_file=$HOME/w261.json \\\n",
    "    --create_new_cluster \\\n",
    "    --pyspark_file=final_proj_GCP_RF_base.py \\\n",
    "    --worker_nodes=6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='w261-fall-2019'\n",
    "ZONE='us-west2-a'\n",
    "CLUSTER_NAME='w261hw5q8'\n",
    "BUCKET_NAME='jyu-mids-w261-2019fall'"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
